import re
import os
import urllib2 
import httplib
import xml.etree.ElementTree as xml
import multiprocessing as mp
from Queue import Queue
import shapefile




def get_record(args):
    '''args = [job_no,link']
    '''
    job_no = args[0]
    link = args[1]
    #print 'getting permit info from link',link
    re_exp = re.compile('expiration',re.IGNORECASE)    
    url = urllib2.urlopen(link)
    lines = url.readlines()
    #--find the line with the info
    for line in lines:
        if re_exp.search(line) is not None:
            approv_date,exp_date,source = parse_content(line)        
    return [job_no,[approv_date,exp_date,source]]

def parse_content(line):    
    re_src = re.compile('source',re.IGNORECASE)
    re_date = re.compile('[0-9]{2}-...-[0-9]{4}')
    dates = re_date.findall(line)
    approv_date = dates[0]
    exp_date = dates[1]    
    raw = re_src.split(line)
    source = raw[1].split('>')[3].split('<')[0]
    return approv_date,exp_date,source

def worker(jobq,resultq,pid):
    #return
    while True:
        args = jobq.get()
        #--check for sentinel
        if args is None:
            print 'process',pid,'sentinel'
            break      
        #print 'process',pid,'getting record',args[0]
        #-- try to get this record, if fails, so what
        try:
            results = get_record(args)
            job_no = results[0]
            approv_date,exp_date,source = results[1]
            f = open('temp\\'+str(job_no)+'.dat','w')
            #print approv_dat+'~'+exp_date+'~'+source+'\n'
            f.write(approv_date+'~'+exp_date+'~'+source+'\n')
            f.close()       
            print 'record',args[0],'done (',pid,')'
        except:
            pass
        #jobq.task_done()
        
    #--sentinel task is done
    #jobq.task_done()     
    return

def main():

    #-----------------------------------------------------------
    #--load the shapefile
    print 'loading shapefile...'
    shp = shapefile.Reader('shapes\\nonpws_centroid_singlepart_domain')
    records = shp.records()
    header = shp.dbfHeader()
    print len(records),' records loaded from shapefile'
    #--find the indexs of important attributes
    idxs = {}
    idxs['link'] = None
    idxs['permit_no'] = None
    idxs['county'] = None
    idxs['final_acti'] = None
    idxs['project_na'] = None
    idxs['app_status'] = None
    for i,h in enumerate(header):
        for k,v in idxs.iteritems():
            if k.upper() == h[0].upper():           
                idxs[k] = i
                break
    for k,v in idxs.iteritems():
        if v == None:
            raise IndexError,'couldnt find index for '+k

    #--build a list of only the records that are needed
    valid_record,valid_shape = [],[]
    counties = ['BROWARD','MIAMI-DADE','PALM BEACH']
    for i,r in enumerate(records):    
        p,c = r[idxs['permit_no']],r[idxs['county']]
        n = r[idxs['project_na']]       
        a_stat = r[idxs['app_status']]        
        if p != '' and c in counties and a_stat.upper() == 'COMPLETE':
            valid_record.append(r)
            valid_shape.append(shp.shape(i))
    

    #--get a unique list of the links that need to be followed for web content
    links = []
    for r in valid_record:
        if r[idxs['link']] not in links:
             links.append(r[idxs['link']])
    print len(links),' records needed'   

    #--get the web info
    print 'retrieving records...'
    #--build the job queue args
    job_no = 0
    queue_args = []
    for r,s in zip(valid_record,valid_shape):
        queue_args.append([job_no,r[idxs['link']]])
        job_no += 1  
        #if job_no >= 100:
        #    break     
    
    #--create queues
    #jobq = mp.JoinableQueue()    
    #resultq = mp.JoinableQueue()           
    # #--add the args to jobq
    #for qa in queue_args:
    #    jobq.put_nowait(qa)
    ##--start the processes    
    #num_procs = 20
    #procs = []
    #for i in range(num_procs):
    #    p = mp.Process(target=worker,args=(jobq,resultq,i+1))
    #    p.daemon = True
    #    p.start()
    #    procs.append(p)                    
    ##--add sentinels...   
    #for i in range(num_procs):
    #    jobq.put(None)
    #jobq.close()                     
    ##--block until all finish
    #for p in procs:
    #    p.join() 
    #    print p.name,'Finished' 
    ##return
    
     #--write a new shapefile
    wr = shapefile.Writer()
    for i,item in enumerate(header):        
        print item[1]
        if item[1] == 'D':
            item[1] = 'C'
            item[2] = 20

        wr.field(item[0],fieldType=item[1],size=item[2],decimal=item[3])
    #--add three new fields for the web content
    wr.field('web_adate',fieldType='C',size='20')
    wr.field('web_edate',fieldType='C',size='20')
    wr.field('web_src',fieldType='C',size='50')
        
    f_fail = open('failed.dat','w')
    c = 0
    for r,s in zip(valid_record,valid_shape):
        idx = None
        link = r[idxs['link']]
        for i,lk in enumerate(links):       
            if link == lk:
                idx = i
                break
        #--find the web result
        fname = 'temp\\'+str(i)+'.dat'
        if not os.path.exists(fname):                        
            f_fail.write(links[i]+'\n')        
        else:
            f = open(fname,'r')
            a_date,e_date,source = f.readline().strip().split('~')
            f.close()           
            r.append(a_date)
            r.append(e_date)
            r.append(source)
            wr.poly([s.points],shapeType=shapefile.POINT)                    
            wr.record(r)
            print 'added record ',c,'of',len(valid_record)
            c += 1
            #break    
    wr.save('shapes\\nonpws_web_points')
    f_fail.close()
            
if __name__ == '__main__':                                                       
    mp.freeze_support() # optional if the program is not frozen                  
    main()   